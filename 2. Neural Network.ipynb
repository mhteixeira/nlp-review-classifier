{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL</th>\n",
       "      <th>REVIEW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRUTHFULPOSITIVE</td>\n",
       "      <td>The sheraton was a wonderful hotel! When me an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRUTHFULPOSITIVE</td>\n",
       "      <td>We stayed at the Omni between Christmas and Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DECEPTIVENEGATIVE</td>\n",
       "      <td>I was REALLY looking forward to a nice relaxin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRUTHFULNEGATIVE</td>\n",
       "      <td>First let me say, I try not to be too critical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DECEPTIVENEGATIVE</td>\n",
       "      <td>The Ambassador East Hotel is a terrible place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>TRUTHFULNEGATIVE</td>\n",
       "      <td>I stayed here for 5 nights last summer. I book...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>TRUTHFULPOSITIVE</td>\n",
       "      <td>Stayed here for 3 nights for a Bridgestone/Fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>TRUTHFULNEGATIVE</td>\n",
       "      <td>I am staying here now and actually am compelle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>TRUTHFULNEGATIVE</td>\n",
       "      <td>We stayed at this hotel with our two teenage d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>DECEPTIVEPOSITIVE</td>\n",
       "      <td>The rooms were beautiful! The staff was friend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  LABEL                                             REVIEW\n",
       "0      TRUTHFULPOSITIVE  The sheraton was a wonderful hotel! When me an...\n",
       "1      TRUTHFULPOSITIVE  We stayed at the Omni between Christmas and Ne...\n",
       "2     DECEPTIVENEGATIVE  I was REALLY looking forward to a nice relaxin...\n",
       "3      TRUTHFULNEGATIVE  First let me say, I try not to be too critical...\n",
       "4     DECEPTIVENEGATIVE  The Ambassador East Hotel is a terrible place ...\n",
       "...                 ...                                                ...\n",
       "1395   TRUTHFULNEGATIVE  I stayed here for 5 nights last summer. I book...\n",
       "1396   TRUTHFULPOSITIVE  Stayed here for 3 nights for a Bridgestone/Fir...\n",
       "1397   TRUTHFULNEGATIVE  I am staying here now and actually am compelle...\n",
       "1398   TRUTHFULNEGATIVE  We stayed at this hotel with our two teenage d...\n",
       "1399  DECEPTIVEPOSITIVE  The rooms were beautiful! The staff was friend...\n",
       "\n",
       "[1400 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./datasets/train.txt\"\n",
    "train = pd.read_csv(file_path, sep='\\t', header=None, names=['LABEL', 'REVIEW'])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REVIEW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I stayed here while we were visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WARNING! My stay at the Talbott Hotel will go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I recently stayed at the Hard Rock Hotel in Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O.M.G best hotel ever ! i've stayed at various...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We became an Ambassador member just before spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>The Millennium Knickerbocker Hotel has seen be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>We got a spanking deal at this hotel for $99 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Just back from a business trip. The Homewood i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>I have just returned from a lovely shopping tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>This hotel had a great location, but you can d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                REVIEW\n",
       "0    My family and I stayed here while we were visi...\n",
       "1    WARNING! My stay at the Talbott Hotel will go ...\n",
       "2    I recently stayed at the Hard Rock Hotel in Ch...\n",
       "3    O.M.G best hotel ever ! i've stayed at various...\n",
       "4    We became an Ambassador member just before spe...\n",
       "..                                                 ...\n",
       "195  The Millennium Knickerbocker Hotel has seen be...\n",
       "196  We got a spanking deal at this hotel for $99 a...\n",
       "197  Just back from a business trip. The Homewood i...\n",
       "198  I have just returned from a lovely shopping tr...\n",
       "199  This hotel had a great location, but you can d...\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./datasets/test_just_reviews.txt\"\n",
    "test = pd.read_csv(file_path, sep='\\t', header=None, names=['REVIEW'])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train['REVIEW'].values)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train['REVIEW'].values)\n",
    "X_test = tokenizer.texts_to_sequences(test['REVIEW'].values)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "\tI was REALLY looking forward to a nice relaxing stay at the end of a long vacation, but unfortunately that was not to be had. From the moment we arrived at the Omni, the staff was belligerent and extremely rude. They had lost our reservation and refused to give us the rate we had booked before. If we weren't so tired, we would've gone to a different hotel right then, but hindsight is 20/20. After we FINALLY got checked in after being made to wait needlessly for 45 minutes in the lobby, we decided to go down to the pool, which was tiny and kind of dirty, so we had walked all the way down there in our suits for nothing. The internet access was really slow. I will NEVER stay here again. Save yourself the headache and book somewhere else!\n",
      "Tokenized vector:\n",
      "\t [5, 6, 96, 188, 617, 3, 4, 63, 510, 29, 14, 1, 433, 9, 4, 198, 426, 25, 526, 17, 6, 19, 3, 33, 22, 35, 1, 690, 10, 130, 14, 1, 311, 1, 42, 6, 2, 249, 220, 26, 22, 691, 27, 169, 2, 1080, 3, 328, 52, 1, 282, 10, 22, 149, 151, 53, 10, 639, 40, 525, 10, 2099, 1012, 3, 4, 348, 8, 154, 163, 25, 16, 460, 460, 66, 10, 187, 80, 177, 7, 66, 201, 102, 3, 193, 11, 1048, 162, 7, 1, 112, 10, 251, 3, 101, 116, 3, 1, 226, 72, 6, 572, 2, 531, 9, 331, 40, 10, 22, 377, 38, 1, 155, 116, 31, 7, 27, 3121, 11, 215, 1, 202, 398, 6, 96, 527, 5, 58, 99, 29, 73, 61, 1111, 1313, 1, 2339, 2, 477, 669, 349]\n",
      "Tokens of the first words:\n",
      "\tI: 5\n",
      "\twas: 6\n",
      "\tREALLY: 96\n",
      "\tlooking: 188\n"
     ]
    }
   ],
   "source": [
    "print('Original text:')\n",
    "print('\\t' + train['REVIEW'].values[2])\n",
    "print('Tokenized vector:')\n",
    "print('\\t', X_train[2])\n",
    "\n",
    "print('Tokens of the first words:')\n",
    "for word in ['I', 'was', 'REALLY', 'looking']:\n",
    "    print('\\t{}: {}'.format(word, tokenizer.word_index[word.lower()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1  419    6    4  173    8   36   43    2   15 2336 2664    7   10\n",
      "   20   96  525   40   10  251    3  172    4  600 2665   10 1010  195\n",
      "    3   56   51    1  158   44  363    3 1538   11    5  293    3  172\n",
      "   13  223   18   43    1   41    6   39    2   21    6  656   45    9\n",
      "    1 1895   53   19    1 1895    8 2666  161   47    7   26   22    4\n",
      "   96   63  135  604   18  190  126    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 400\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 400, 50)           458900    \n",
      "                                                                 \n",
      " global_max_pooling1d_6 (Glo  (None, 50)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                510       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 459,454\n",
      "Trainable params: 459,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(train['LABEL'].values)\n",
    "labels = to_categorical(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 11s 33ms/step - loss: 1.3720 - accuracy: 0.3187 - val_loss: 1.3455 - val_accuracy: 0.4500\n",
      "Epoch 2/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 1.2847 - accuracy: 0.4741 - val_loss: 1.2210 - val_accuracy: 0.4786\n",
      "Epoch 3/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 1.0875 - accuracy: 0.5107 - val_loss: 1.0248 - val_accuracy: 0.5393\n",
      "Epoch 4/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.8754 - accuracy: 0.6268 - val_loss: 0.8856 - val_accuracy: 0.6643\n",
      "Epoch 5/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.7153 - accuracy: 0.7884 - val_loss: 0.7842 - val_accuracy: 0.7250\n",
      "Epoch 6/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.5732 - accuracy: 0.8696 - val_loss: 0.7075 - val_accuracy: 0.7500\n",
      "Epoch 7/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.4394 - accuracy: 0.9187 - val_loss: 0.6464 - val_accuracy: 0.7464\n",
      "Epoch 8/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.3234 - accuracy: 0.9500 - val_loss: 0.6027 - val_accuracy: 0.7786\n",
      "Epoch 9/20\n",
      "112/112 [==============================] - 3s 25ms/step - loss: 0.2322 - accuracy: 0.9705 - val_loss: 0.5748 - val_accuracy: 0.7714\n",
      "Epoch 10/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.1651 - accuracy: 0.9812 - val_loss: 0.5587 - val_accuracy: 0.7786\n",
      "Epoch 11/20\n",
      "112/112 [==============================] - 3s 29ms/step - loss: 0.1161 - accuracy: 0.9893 - val_loss: 0.5533 - val_accuracy: 0.7857\n",
      "Epoch 12/20\n",
      "112/112 [==============================] - 2s 19ms/step - loss: 0.0837 - accuracy: 0.9964 - val_loss: 0.5514 - val_accuracy: 0.7857\n",
      "Epoch 13/20\n",
      "112/112 [==============================] - 2s 18ms/step - loss: 0.0616 - accuracy: 0.9973 - val_loss: 0.5534 - val_accuracy: 0.7929\n",
      "Epoch 14/20\n",
      "112/112 [==============================] - 2s 17ms/step - loss: 0.0466 - accuracy: 0.9991 - val_loss: 0.5552 - val_accuracy: 0.7857\n",
      "Epoch 15/20\n",
      "112/112 [==============================] - 2s 17ms/step - loss: 0.0357 - accuracy: 0.9991 - val_loss: 0.5645 - val_accuracy: 0.7857\n",
      "Epoch 16/20\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 0.0280 - accuracy: 0.9991 - val_loss: 0.5681 - val_accuracy: 0.7964\n",
      "Epoch 17/20\n",
      "112/112 [==============================] - 2s 22ms/step - loss: 0.0226 - accuracy: 0.9991 - val_loss: 0.5769 - val_accuracy: 0.7893\n",
      "Epoch 18/20\n",
      "112/112 [==============================] - 2s 22ms/step - loss: 0.0183 - accuracy: 0.9991 - val_loss: 0.5848 - val_accuracy: 0.7929\n",
      "Epoch 19/20\n",
      "112/112 [==============================] - 3s 25ms/step - loss: 0.0146 - accuracy: 0.9991 - val_loss: 0.5904 - val_accuracy: 0.7857\n",
      "Epoch 20/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 0.7857\n",
      "Epoch 1/20\n",
      "112/112 [==============================] - 8s 30ms/step - loss: 1.3773 - accuracy: 0.3241 - val_loss: 1.3636 - val_accuracy: 0.3893\n",
      "Epoch 2/20\n",
      "112/112 [==============================] - 3s 26ms/step - loss: 1.3208 - accuracy: 0.4938 - val_loss: 1.2864 - val_accuracy: 0.4964\n",
      "Epoch 3/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 1.1641 - accuracy: 0.6652 - val_loss: 1.1417 - val_accuracy: 0.5250\n",
      "Epoch 4/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.9412 - accuracy: 0.6920 - val_loss: 0.9763 - val_accuracy: 0.5929\n",
      "Epoch 5/20\n",
      "112/112 [==============================] - 3s 26ms/step - loss: 0.7494 - accuracy: 0.7982 - val_loss: 0.8715 - val_accuracy: 0.6107\n",
      "Epoch 6/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.5953 - accuracy: 0.8473 - val_loss: 0.7820 - val_accuracy: 0.6643\n",
      "Epoch 7/20\n",
      "112/112 [==============================] - 3s 25ms/step - loss: 0.4680 - accuracy: 0.8964 - val_loss: 0.7141 - val_accuracy: 0.7143\n",
      "Epoch 8/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.3610 - accuracy: 0.9277 - val_loss: 0.7024 - val_accuracy: 0.6857\n",
      "Epoch 9/20\n",
      "112/112 [==============================] - 3s 25ms/step - loss: 0.2714 - accuracy: 0.9536 - val_loss: 0.6345 - val_accuracy: 0.7464\n",
      "Epoch 10/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.2031 - accuracy: 0.9714 - val_loss: 0.6215 - val_accuracy: 0.7500\n",
      "Epoch 11/20\n",
      "112/112 [==============================] - 2s 22ms/step - loss: 0.1505 - accuracy: 0.9839 - val_loss: 0.6045 - val_accuracy: 0.7607\n",
      "Epoch 12/20\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 0.1108 - accuracy: 0.9911 - val_loss: 0.6022 - val_accuracy: 0.7571\n",
      "Epoch 13/20\n",
      "112/112 [==============================] - 2s 18ms/step - loss: 0.0825 - accuracy: 0.9973 - val_loss: 0.6065 - val_accuracy: 0.7571\n",
      "Epoch 14/20\n",
      "112/112 [==============================] - 2s 17ms/step - loss: 0.0626 - accuracy: 0.9973 - val_loss: 0.6210 - val_accuracy: 0.7607\n",
      "Epoch 15/20\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 0.0478 - accuracy: 0.9973 - val_loss: 0.6151 - val_accuracy: 0.7607\n",
      "Epoch 16/20\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 0.0369 - accuracy: 0.9991 - val_loss: 0.6147 - val_accuracy: 0.7571\n",
      "Epoch 17/20\n",
      "112/112 [==============================] - 3s 25ms/step - loss: 0.0288 - accuracy: 0.9991 - val_loss: 0.6206 - val_accuracy: 0.7536\n",
      "Epoch 18/20\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 0.0229 - accuracy: 0.9991 - val_loss: 0.6311 - val_accuracy: 0.7500\n",
      "Epoch 19/20\n",
      "112/112 [==============================] - 3s 26ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.6303 - val_accuracy: 0.7536\n",
      "Epoch 20/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.6493 - val_accuracy: 0.7500\n",
      "Epoch 1/20\n",
      "112/112 [==============================] - 9s 34ms/step - loss: 1.3812 - accuracy: 0.3375 - val_loss: 1.3692 - val_accuracy: 0.3964\n",
      "Epoch 2/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 1.3244 - accuracy: 0.5527 - val_loss: 1.2818 - val_accuracy: 0.5321\n",
      "Epoch 3/20\n",
      "112/112 [==============================] - 3s 26ms/step - loss: 1.1494 - accuracy: 0.6607 - val_loss: 1.1043 - val_accuracy: 0.5393\n",
      "Epoch 4/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.9060 - accuracy: 0.7134 - val_loss: 0.9531 - val_accuracy: 0.5964\n",
      "Epoch 5/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.7019 - accuracy: 0.8116 - val_loss: 0.8498 - val_accuracy: 0.6536\n",
      "Epoch 6/20\n",
      "112/112 [==============================] - 3s 26ms/step - loss: 0.5420 - accuracy: 0.8786 - val_loss: 0.7943 - val_accuracy: 0.6536\n",
      "Epoch 7/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.4113 - accuracy: 0.9196 - val_loss: 0.7508 - val_accuracy: 0.6893\n",
      "Epoch 8/20\n",
      "112/112 [==============================] - 2s 22ms/step - loss: 0.3100 - accuracy: 0.9500 - val_loss: 0.7194 - val_accuracy: 0.7071\n",
      "Epoch 9/20\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 0.2312 - accuracy: 0.9679 - val_loss: 0.7004 - val_accuracy: 0.7143\n",
      "Epoch 10/20\n",
      "112/112 [==============================] - 2s 22ms/step - loss: 0.1720 - accuracy: 0.9812 - val_loss: 0.7087 - val_accuracy: 0.6893\n",
      "Epoch 11/20\n",
      "112/112 [==============================] - 2s 19ms/step - loss: 0.1277 - accuracy: 0.9911 - val_loss: 0.7037 - val_accuracy: 0.7036\n",
      "Epoch 12/20\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 0.0964 - accuracy: 0.9955 - val_loss: 0.6890 - val_accuracy: 0.7071\n",
      "Epoch 13/20\n",
      "112/112 [==============================] - 2s 14ms/step - loss: 0.0737 - accuracy: 0.9991 - val_loss: 0.6929 - val_accuracy: 0.7036\n",
      "Epoch 14/20\n",
      "112/112 [==============================] - 2s 14ms/step - loss: 0.0569 - accuracy: 0.9991 - val_loss: 0.6945 - val_accuracy: 0.7000\n",
      "Epoch 15/20\n",
      "112/112 [==============================] - 2s 14ms/step - loss: 0.0452 - accuracy: 0.9991 - val_loss: 0.6970 - val_accuracy: 0.7071\n",
      "Epoch 16/20\n",
      "112/112 [==============================] - 2s 19ms/step - loss: 0.0359 - accuracy: 0.9991 - val_loss: 0.7009 - val_accuracy: 0.7000\n",
      "Epoch 17/20\n",
      "112/112 [==============================] - 2s 22ms/step - loss: 0.0287 - accuracy: 0.9991 - val_loss: 0.7112 - val_accuracy: 0.7036\n",
      "Epoch 18/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.0232 - accuracy: 0.9991 - val_loss: 0.7194 - val_accuracy: 0.6929\n",
      "Epoch 19/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.0190 - accuracy: 0.9991 - val_loss: 0.7280 - val_accuracy: 0.7071\n",
      "Epoch 20/20\n",
      "112/112 [==============================] - 3s 25ms/step - loss: 0.0156 - accuracy: 0.9991 - val_loss: 0.7291 - val_accuracy: 0.7036\n",
      "Epoch 1/20\n",
      "112/112 [==============================] - 10s 40ms/step - loss: 1.3773 - accuracy: 0.2857 - val_loss: 1.3644 - val_accuracy: 0.2929\n",
      "Epoch 2/20\n",
      "112/112 [==============================] - 3s 27ms/step - loss: 1.3099 - accuracy: 0.4920 - val_loss: 1.2614 - val_accuracy: 0.5714\n",
      "Epoch 3/20\n",
      "112/112 [==============================] - 3s 26ms/step - loss: 1.1166 - accuracy: 0.7152 - val_loss: 1.0490 - val_accuracy: 0.6321\n",
      "Epoch 4/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.8289 - accuracy: 0.8152 - val_loss: 0.8452 - val_accuracy: 0.7071\n",
      "Epoch 5/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.5765 - accuracy: 0.8866 - val_loss: 0.7106 - val_accuracy: 0.7393\n",
      "Epoch 6/20\n",
      "112/112 [==============================] - 2s 22ms/step - loss: 0.4026 - accuracy: 0.9241 - val_loss: 0.6419 - val_accuracy: 0.7607\n",
      "Epoch 7/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.2799 - accuracy: 0.9563 - val_loss: 0.6145 - val_accuracy: 0.7571\n",
      "Epoch 8/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.1977 - accuracy: 0.9741 - val_loss: 0.5785 - val_accuracy: 0.7714\n",
      "Epoch 9/20\n",
      "112/112 [==============================] - 3s 26ms/step - loss: 0.1392 - accuracy: 0.9848 - val_loss: 0.5676 - val_accuracy: 0.7714\n",
      "Epoch 10/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.0988 - accuracy: 0.9937 - val_loss: 0.5656 - val_accuracy: 0.7821\n",
      "Epoch 11/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.0710 - accuracy: 0.9964 - val_loss: 0.5819 - val_accuracy: 0.7679\n",
      "Epoch 12/20\n",
      "112/112 [==============================] - 2s 19ms/step - loss: 0.0524 - accuracy: 0.9991 - val_loss: 0.5724 - val_accuracy: 0.7750\n",
      "Epoch 13/20\n",
      "112/112 [==============================] - 2s 17ms/step - loss: 0.0397 - accuracy: 0.9991 - val_loss: 0.5788 - val_accuracy: 0.7786\n",
      "Epoch 14/20\n",
      "112/112 [==============================] - 2s 17ms/step - loss: 0.0305 - accuracy: 0.9991 - val_loss: 0.5882 - val_accuracy: 0.7714\n",
      "Epoch 15/20\n",
      "112/112 [==============================] - 2s 18ms/step - loss: 0.0237 - accuracy: 0.9991 - val_loss: 0.5910 - val_accuracy: 0.7714\n",
      "Epoch 16/20\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 0.0186 - accuracy: 0.9991 - val_loss: 0.5985 - val_accuracy: 0.7679\n",
      "Epoch 17/20\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.6038 - val_accuracy: 0.7714\n",
      "Epoch 18/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 0.7679\n",
      "Epoch 19/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.6216 - val_accuracy: 0.7643\n",
      "Epoch 20/20\n",
      "112/112 [==============================] - 2s 22ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.6270 - val_accuracy: 0.7750\n",
      "Epoch 1/20\n",
      "112/112 [==============================] - 9s 39ms/step - loss: 1.3724 - accuracy: 0.3482 - val_loss: 1.3508 - val_accuracy: 0.4464\n",
      "Epoch 2/20\n",
      "112/112 [==============================] - 3s 26ms/step - loss: 1.2857 - accuracy: 0.5107 - val_loss: 1.2297 - val_accuracy: 0.5107\n",
      "Epoch 3/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 1.0923 - accuracy: 0.6116 - val_loss: 1.0463 - val_accuracy: 0.5321\n",
      "Epoch 4/20\n",
      "112/112 [==============================] - 3s 25ms/step - loss: 0.8661 - accuracy: 0.7241 - val_loss: 0.8984 - val_accuracy: 0.6321\n",
      "Epoch 5/20\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 0.6763 - accuracy: 0.8205 - val_loss: 0.8016 - val_accuracy: 0.6607\n",
      "Epoch 6/20\n",
      "112/112 [==============================] - 3s 25ms/step - loss: 0.5140 - accuracy: 0.8955 - val_loss: 0.7258 - val_accuracy: 0.6750\n",
      "Epoch 7/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.3795 - accuracy: 0.9304 - val_loss: 0.6877 - val_accuracy: 0.7036\n",
      "Epoch 8/20\n",
      "112/112 [==============================] - 3s 23ms/step - loss: 0.2727 - accuracy: 0.9554 - val_loss: 0.6682 - val_accuracy: 0.7321\n",
      "Epoch 9/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.1932 - accuracy: 0.9768 - val_loss: 0.6534 - val_accuracy: 0.7250\n",
      "Epoch 10/20\n",
      "112/112 [==============================] - 3s 24ms/step - loss: 0.1360 - accuracy: 0.9875 - val_loss: 0.6589 - val_accuracy: 0.7357\n",
      "Epoch 11/20\n",
      "112/112 [==============================] - 2s 20ms/step - loss: 0.0951 - accuracy: 0.9982 - val_loss: 0.6596 - val_accuracy: 0.7179\n",
      "Epoch 12/20\n",
      "112/112 [==============================] - 2s 18ms/step - loss: 0.0676 - accuracy: 0.9991 - val_loss: 0.6610 - val_accuracy: 0.7357\n",
      "Epoch 13/20\n",
      "112/112 [==============================] - 2s 17ms/step - loss: 0.0487 - accuracy: 1.0000 - val_loss: 0.6702 - val_accuracy: 0.7321\n",
      "Epoch 14/20\n",
      "112/112 [==============================] - 2s 18ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.6776 - val_accuracy: 0.7179\n",
      "Epoch 15/20\n",
      "112/112 [==============================] - 2s 15ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.6871 - val_accuracy: 0.7143\n",
      "Epoch 16/20\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.6990 - val_accuracy: 0.7143\n",
      "Epoch 17/20\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.7081 - val_accuracy: 0.7107\n",
      "Epoch 18/20\n",
      "112/112 [==============================] - 2s 15ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.7168 - val_accuracy: 0.7143\n",
      "Epoch 19/20\n",
      "112/112 [==============================] - 2s 15ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.7263 - val_accuracy: 0.7179\n",
      "Epoch 20/20\n",
      "112/112 [==============================] - 2s 15ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.7346 - val_accuracy: 0.7179\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.7857142686843872, 0.75, 0.7035714387893677, 0.7749999761581421, 0.7178571224212646]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for train_index, val_index in kf.split(X_train, labels):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                            output_dim=embedding_dim, \n",
    "                            input_length=maxlen))\n",
    "    model.add(layers.GlobalMaxPool1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(4, activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    training_data = X_train[train_index]\n",
    "    training_labels = labels[train_index]\n",
    "    validation_data = X_train[val_index]\n",
    "    validation_labels = labels[val_index]\n",
    "    model.fit(training_data, training_labels, epochs=20, validation_data=(validation_data, validation_labels), verbose=True, batch_size=10)\n",
    "    _, accuracy = model.evaluate(training_data, training_labels, verbose=False)\n",
    "    train_accs.append(accuracy)\n",
    "    _, accuracy = model.evaluate(validation_data, validation_labels, verbose=False)\n",
    "    val_accs.append(accuracy)\n",
    "print(train_accs)\n",
    "print(val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7464285612106323"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_class_labels = encoder.inverse_transform(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"results/NN.txt\", \"w\")\n",
    "for s in predicted_class_labels:\n",
    "    f.write(s + '\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
